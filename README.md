# CS285
This repository is my solution to the homework(1-2) given in the berkeley CS285 deep RL course.  
In this repository you can explenations on the algorithms used, full implementation code, results and how to reproduce the results shown. 

The base code of this repository is from: https://github.com/berkeleydeeprlcourse/homework_fall2019. The code written here heavely relies on that repository.
The homework included in the repo:
1. Imitation learning
2. Policy Gradients
3. Q learning and Actor Critic

# SOME RESULTS
Result from the first homework on imitation learning:

![Results after 200 epochs](https://github.com/zoharri/CS285/blob/master/HW1%20-%20Imitation%20learning/results/Humanoid200.gif)

Result from the third homework on Q learning:

![Results after 95 epochs](https://github.com/zoharri/CS285/blob/master/HW3%20-%20Q%20learning%20and%20Actor%20Critic/cs285/data/dqn_q1_PongNoFrameskip-v4_09-04-2020_21-44-30/gym/PongSum.gif)

Policy result after 5 epochs:

![Results after 5 epochs](https://github.com/zoharri/CS285/blob/master/HW1%20-%20Imitation%20learning/results/Humanoid5.gif)

Policy result after 95 epochs:

![Results after 95 epochs](https://github.com/zoharri/CS285/blob/master/HW1%20-%20Imitation%20learning/results/Humanoid95.gif)

Policy result after200 epochs:

![Results after 95 epochs](https://github.com/zoharri/CS285/blob/master/HW1%20-%20Imitation%20learning/results/Humanoid200.gif)

<!-- GETTING STARTED -->
# GETTING STARTED
1. Download mujoco200 (not mujoco150 as stated in the original repo)
2. I recommend using conda, create the new env with: "conda env create -f cs285_env.yml" make sure that the path in written in the prefix section inside the yml file is correct
3. The rest of the insructions are the same as the original repo (run the pip install -e . and setup file) 


<!-- CONTACT -->
# CONTACT
Zohar Rimon - zohar.rimon@campus.technion.ac.il

Project Link: [https://github.com/zoharri/CS285](https://github.com/zoharri/CS285)





